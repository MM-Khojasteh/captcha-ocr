{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR Libraries Benchmark: Comprehensive Performance Analysis\n",
    "\n",
    "## Overview\n",
    "This notebook provides a detailed benchmark comparison of popular OCR (Optical Character Recognition) libraries:\n",
    "- **Pytesseract** - Google's Tesseract OCR Engine\n",
    "- **EasyOCR** - Ready-to-use OCR with 80+ language support\n",
    "- **Keras-OCR** - OCR pipeline built with Keras\n",
    "- **TrOCR** - Transformer-based OCR from Microsoft\n",
    "- **docTR** - Document Text Recognition library\n",
    "\n",
    "## Evaluation Metrics\n",
    "- Character Error Rate (CER)\n",
    "- Word Error Rate (WER)\n",
    "- Processing Speed\n",
    "- Memory Usage\n",
    "- Language Support\n",
    "- Accuracy on different text types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T09:16:55.982944Z",
     "start_time": "2025-10-16T09:16:45.274854Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "WARNING: python-doctr 1.0.0 does not provide the extra 'torch'\n",
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -q pytesseract pillow\n",
    "!pip install -q easyocr\n",
    "!pip install -q keras-ocr\n",
    "!pip install -q transformers torch\n",
    "!pip install -q python-doctr[torch]\n",
    "!pip install -q pandas matplotlib seaborn tqdm\n",
    "!pip install -q Levenshtein memory-profiler\n",
    "\n",
    "# For Tesseract, you may need to install the engine separately:\n",
    "# Ubuntu/Debian: sudo apt-get install tesseract-ocr\n",
    "# Windows: Download from https://github.com/UB-Mannheim/tesseract/wiki\n",
    "# macOS: brew install tesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T09:16:59.212766Z",
     "start_time": "2025-10-16T09:16:56.037846Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Any\n",
    "from dataclasses import dataclass, field\n",
    "from collections import defaultdict\n",
    "import traceback\n",
    "import psutil\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import cv2\n",
    "\n",
    "# Metrics\n",
    "import Levenshtein\n",
    "from memory_profiler import memory_usage\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Benchmark Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T09:16:59.242900Z",
     "start_time": "2025-10-16T09:16:59.218307Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BenchmarkConfig:\n",
    "    \"\"\"Configuration for benchmark tests\"\"\"\n",
    "    test_categories: List[str] = field(default_factory=lambda: [\n",
    "        'printed_text',\n",
    "        'handwritten',\n",
    "        'scene_text',\n",
    "        'distorted_text',\n",
    "        'multi_language',\n",
    "        'low_quality'\n",
    "    ])\n",
    "    \n",
    "    image_sizes: List[Tuple[int, int]] = field(default_factory=lambda: [\n",
    "        (640, 480),\n",
    "        (1280, 720),\n",
    "        (1920, 1080)\n",
    "    ])\n",
    "    \n",
    "    languages: List[str] = field(default_factory=lambda: ['en', 'fr', 'de', 'es', 'zh'])\n",
    "    \n",
    "    metrics: List[str] = field(default_factory=lambda: [\n",
    "        'accuracy',\n",
    "        'cer',  # Character Error Rate\n",
    "        'wer',  # Word Error Rate\n",
    "        'processing_time',\n",
    "        'memory_usage',\n",
    "        'confidence_score'\n",
    "    ])\n",
    "    \n",
    "    output_dir: Path = Path('./benchmark_results')\n",
    "    data_dir: Path = Path('./benchmark_data')\n",
    "\n",
    "config = BenchmarkConfig()\n",
    "config.output_dir.mkdir(exist_ok=True)\n",
    "config.data_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T09:17:00.219174Z",
     "start_time": "2025-10-16T09:16:59.267046Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 44 test images\n",
      "category   distortion\n",
      "complex    blur          4\n",
      "           noise         4\n",
      "           none          4\n",
      "           rotation      4\n",
      "paragraph  blur          2\n",
      "           noise         2\n",
      "           none          2\n",
      "           rotation      2\n",
      "simple     blur          5\n",
      "           noise         5\n",
      "           none          5\n",
      "           rotation      5\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "class TestDataGenerator:\n",
    "    \"\"\"Generate synthetic test data for OCR benchmarking\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir: Path):\n",
    "        self.output_dir = output_dir\n",
    "        self.test_texts = {\n",
    "            'simple': [\n",
    "                \"The quick brown fox jumps over the lazy dog\",\n",
    "                \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\",\n",
    "                \"abcdefghijklmnopqrstuvwxyz\",\n",
    "                \"0123456789\",\n",
    "                \"Hello World! This is a test.\"\n",
    "            ],\n",
    "            'complex': [\n",
    "                \"Email: user@example.com | Phone: +1-234-567-8900\",\n",
    "                \"Price: $99.99 (Save 20%!)\",\n",
    "                \"Date: 2024-01-15 Time: 14:30:00\",\n",
    "                \"Special chars: @#$%^&*()_+-=[]{}|;:',.<>?/\"\n",
    "            ],\n",
    "            'paragraph': [\n",
    "                \"\"\"Lorem ipsum dolor sit amet, consectetur adipiscing elit. \n",
    "                Sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\"\"\",\n",
    "                \"\"\"Machine learning is a subset of artificial intelligence that \n",
    "                enables systems to learn and improve from experience.\"\"\"\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def create_text_image(self, text: str, font_size: int = 24, \n",
    "                         distortion: str = None, image_size: Tuple[int, int] = (800, 200)):\n",
    "        \"\"\"Create an image with text for testing\"\"\"\n",
    "        img = Image.new('RGB', image_size, color='white')\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        \n",
    "        # Try to use a font, fallback to default if not available\n",
    "        try:\n",
    "            font = ImageFont.truetype(\"arial.ttf\", font_size)\n",
    "        except:\n",
    "            font = ImageFont.load_default()\n",
    "        \n",
    "        # Calculate text position\n",
    "        text_bbox = draw.textbbox((0, 0), text, font=font)\n",
    "        text_width = text_bbox[2] - text_bbox[0]\n",
    "        text_height = text_bbox[3] - text_bbox[1]\n",
    "        \n",
    "        x = (image_size[0] - text_width) // 2\n",
    "        y = (image_size[1] - text_height) // 2\n",
    "        \n",
    "        draw.text((x, y), text, fill='black', font=font)\n",
    "        \n",
    "        # Apply distortions if specified\n",
    "        if distortion:\n",
    "            img = self.apply_distortion(img, distortion)\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def apply_distortion(self, img: Image.Image, distortion_type: str) -> Image.Image:\n",
    "        \"\"\"Apply various distortions to test robustness\"\"\"\n",
    "        img_array = np.array(img)\n",
    "        \n",
    "        if distortion_type == 'noise':\n",
    "            noise = np.random.normal(0, 25, img_array.shape).astype(np.uint8)\n",
    "            img_array = cv2.add(img_array, noise)\n",
    "        \n",
    "        elif distortion_type == 'blur':\n",
    "            img_array = cv2.GaussianBlur(img_array, (5, 5), 0)\n",
    "        \n",
    "        elif distortion_type == 'rotation':\n",
    "            angle = np.random.uniform(-5, 5)\n",
    "            center = (img_array.shape[1]//2, img_array.shape[0]//2)\n",
    "            matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "            img_array = cv2.warpAffine(img_array, matrix, \n",
    "                                      (img_array.shape[1], img_array.shape[0]),\n",
    "                                      borderValue=(255, 255, 255))\n",
    "        \n",
    "        elif distortion_type == 'perspective':\n",
    "            h, w = img_array.shape[:2]\n",
    "            pts1 = np.float32([[0, 0], [w, 0], [0, h], [w, h]])\n",
    "            pts2 = np.float32([[0, 0], [w, 0], \n",
    "                              [int(0.1*w), h], \n",
    "                              [int(0.9*w), h]])\n",
    "            matrix = cv2.getPerspectiveTransform(pts1, pts2)\n",
    "            img_array = cv2.warpPerspective(img_array, matrix, (w, h),\n",
    "                                           borderValue=(255, 255, 255))\n",
    "        \n",
    "        return Image.fromarray(img_array)\n",
    "    \n",
    "    def generate_test_dataset(self):\n",
    "        \"\"\"Generate complete test dataset\"\"\"\n",
    "        test_data = []\n",
    "        \n",
    "        for category, texts in self.test_texts.items():\n",
    "            category_dir = self.output_dir / category\n",
    "            category_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            for i, text in enumerate(texts):\n",
    "                # Normal image\n",
    "                img = self.create_text_image(text)\n",
    "                img_path = category_dir / f\"{category}_{i}_normal.png\"\n",
    "                img.save(img_path)\n",
    "                test_data.append({\n",
    "                    'path': str(img_path),\n",
    "                    'text': text,\n",
    "                    'category': category,\n",
    "                    'distortion': 'none'\n",
    "                })\n",
    "                \n",
    "                # Distorted versions\n",
    "                for distortion in ['noise', 'blur', 'rotation']:\n",
    "                    img = self.create_text_image(text, distortion=distortion)\n",
    "                    img_path = category_dir / f\"{category}_{i}_{distortion}.png\"\n",
    "                    img.save(img_path)\n",
    "                    test_data.append({\n",
    "                        'path': str(img_path),\n",
    "                        'text': text,\n",
    "                        'category': category,\n",
    "                        'distortion': distortion\n",
    "                    })\n",
    "        \n",
    "        return pd.DataFrame(test_data)\n",
    "\n",
    "# Generate test data\n",
    "generator = TestDataGenerator(config.data_dir)\n",
    "test_df = generator.generate_test_dataset()\n",
    "print(f\"Generated {len(test_df)} test images\")\n",
    "print(test_df.groupby(['category', 'distortion']).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T09:17:00.280555Z",
     "start_time": "2025-10-16T09:17:00.243904Z"
    }
   },
   "outputs": [],
   "source": [
    "class OCRMetrics:\n",
    "    \"\"\"Calculate various OCR performance metrics\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def character_error_rate(reference: str, hypothesis: str) -> float:\n",
    "        \"\"\"Calculate Character Error Rate (CER)\"\"\"\n",
    "        if len(reference) == 0:\n",
    "            return 0.0 if len(hypothesis) == 0 else 1.0\n",
    "        \n",
    "        distance = Levenshtein.distance(reference, hypothesis)\n",
    "        return distance / len(reference)\n",
    "    \n",
    "    @staticmethod\n",
    "    def word_error_rate(reference: str, hypothesis: str) -> float:\n",
    "        \"\"\"Calculate Word Error Rate (WER)\"\"\"\n",
    "        ref_words = reference.split()\n",
    "        hyp_words = hypothesis.split()\n",
    "        \n",
    "        if len(ref_words) == 0:\n",
    "            return 0.0 if len(hyp_words) == 0 else 1.0\n",
    "        \n",
    "        distance = Levenshtein.distance(ref_words, hyp_words)\n",
    "        return distance / len(ref_words)\n",
    "    \n",
    "    @staticmethod\n",
    "    def accuracy(reference: str, hypothesis: str) -> float:\n",
    "        \"\"\"Calculate exact match accuracy\"\"\"\n",
    "        return 1.0 if reference == hypothesis else 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def precision_recall_f1(reference: str, hypothesis: str) -> Dict[str, float]:\n",
    "        \"\"\"Calculate precision, recall, and F1 score at character level\"\"\"\n",
    "        ref_chars = set(reference)\n",
    "        hyp_chars = set(hypothesis)\n",
    "        \n",
    "        if len(hyp_chars) == 0:\n",
    "            return {'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n",
    "        \n",
    "        true_positives = len(ref_chars.intersection(hyp_chars))\n",
    "        \n",
    "        precision = true_positives / len(hyp_chars) if len(hyp_chars) > 0 else 0\n",
    "        recall = true_positives / len(ref_chars) if len(ref_chars) > 0 else 0\n",
    "        \n",
    "        if precision + recall == 0:\n",
    "            f1 = 0\n",
    "        else:\n",
    "            f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        \n",
    "        return {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1\n",
    "        }\n",
    "\n",
    "metrics = OCRMetrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. OCR Library Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T09:17:00.317892Z",
     "start_time": "2025-10-16T09:17:00.299700Z"
    }
   },
   "outputs": [],
   "source": [
    "class OCRWrapper:\n",
    "    \"\"\"Base class for OCR library wrappers\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.is_initialized = False\n",
    "    \n",
    "    def initialize(self):\n",
    "        \"\"\"Initialize the OCR engine\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def process_image(self, image_path: str) -> Dict[str, Any]:\n",
    "        \"\"\"Process an image and return OCR results\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Cleanup resources\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T09:17:00.347929Z",
     "start_time": "2025-10-16T09:17:00.329846Z"
    }
   },
   "outputs": [],
   "source": [
    "class PytesseractWrapper(OCRWrapper):\n",
    "    \"\"\"Wrapper for Pytesseract\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"Pytesseract\")\n",
    "    \n",
    "    def initialize(self):\n",
    "        try:\n",
    "            import pytesseract\n",
    "            self.pytesseract = pytesseract\n",
    "            # Test if tesseract is installed\n",
    "            self.pytesseract.get_tesseract_version()\n",
    "            self.is_initialized = True\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to initialize Pytesseract: {e}\")\n",
    "            self.is_initialized = False\n",
    "    \n",
    "    def process_image(self, image_path: str) -> Dict[str, Any]:\n",
    "        if not self.is_initialized:\n",
    "            return {'text': '', 'confidence': 0, 'error': 'Not initialized'}\n",
    "        \n",
    "        try:\n",
    "            img = Image.open(image_path)\n",
    "            \n",
    "            # Get text with confidence scores\n",
    "            data = self.pytesseract.image_to_data(img, output_type=self.pytesseract.Output.DICT)\n",
    "            \n",
    "            # Extract text and calculate average confidence\n",
    "            words = []\n",
    "            confidences = []\n",
    "            \n",
    "            for i, word in enumerate(data['text']):\n",
    "                if word.strip():\n",
    "                    words.append(word)\n",
    "                    conf = data['conf'][i]\n",
    "                    if conf > 0:  # Tesseract uses -1 for no confidence\n",
    "                        confidences.append(conf)\n",
    "            \n",
    "            text = ' '.join(words)\n",
    "            avg_confidence = np.mean(confidences) if confidences else 0\n",
    "            \n",
    "            return {\n",
    "                'text': text,\n",
    "                'confidence': avg_confidence / 100,  # Normalize to 0-1\n",
    "                'raw_data': data\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {'text': '', 'confidence': 0, 'error': str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T09:17:00.373776Z",
     "start_time": "2025-10-16T09:17:00.356196Z"
    }
   },
   "outputs": [],
   "source": [
    "class EasyOCRWrapper(OCRWrapper):\n",
    "    \"\"\"Wrapper for EasyOCR\"\"\"\n",
    "    \n",
    "    def __init__(self, languages=['en']):\n",
    "        super().__init__(\"EasyOCR\")\n",
    "        self.languages = languages\n",
    "    \n",
    "    def initialize(self):\n",
    "        try:\n",
    "            import easyocr\n",
    "            self.reader = easyocr.Reader(self.languages, gpu=False)\n",
    "            self.is_initialized = True\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to initialize EasyOCR: {e}\")\n",
    "            self.is_initialized = False\n",
    "    \n",
    "    def process_image(self, image_path: str) -> Dict[str, Any]:\n",
    "        if not self.is_initialized:\n",
    "            return {'text': '', 'confidence': 0, 'error': 'Not initialized'}\n",
    "        \n",
    "        try:\n",
    "            results = self.reader.readtext(image_path)\n",
    "            \n",
    "            if not results:\n",
    "                return {'text': '', 'confidence': 0}\n",
    "            \n",
    "            texts = []\n",
    "            confidences = []\n",
    "            \n",
    "            for (bbox, text, confidence) in results:\n",
    "                texts.append(text)\n",
    "                confidences.append(confidence)\n",
    "            \n",
    "            full_text = ' '.join(texts)\n",
    "            avg_confidence = np.mean(confidences) if confidences else 0\n",
    "            \n",
    "            return {\n",
    "                'text': full_text,\n",
    "                'confidence': avg_confidence,\n",
    "                'boxes': results\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {'text': '', 'confidence': 0, 'error': str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T09:17:00.396742Z",
     "start_time": "2025-10-16T09:17:00.384208Z"
    }
   },
   "outputs": [],
   "source": [
    "class KerasOCRWrapper(OCRWrapper):\n",
    "    \"\"\"Wrapper for Keras-OCR\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"Keras-OCR\")\n",
    "    \n",
    "    def initialize(self):\n",
    "        try:\n",
    "            import keras_ocr\n",
    "            self.pipeline = keras_ocr.pipeline.Pipeline()\n",
    "            self.is_initialized = True\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to initialize Keras-OCR: {e}\")\n",
    "            self.is_initialized = False\n",
    "    \n",
    "    def process_image(self, image_path: str) -> Dict[str, Any]:\n",
    "        if not self.is_initialized:\n",
    "            return {'text': '', 'confidence': 0, 'error': 'Not initialized'}\n",
    "        \n",
    "        try:\n",
    "            import keras_ocr\n",
    "            \n",
    "            # Read image\n",
    "            image = keras_ocr.tools.read(image_path)\n",
    "            \n",
    "            # Get predictions\n",
    "            prediction_groups = self.pipeline.recognize([image])\n",
    "            \n",
    "            if not prediction_groups or not prediction_groups[0]:\n",
    "                return {'text': '', 'confidence': 0}\n",
    "            \n",
    "            # Extract text from predictions\n",
    "            predictions = prediction_groups[0]\n",
    "            texts = [text for text, box in predictions]\n",
    "            \n",
    "            # Keras-OCR doesn't provide confidence scores directly\n",
    "            # We'll use a placeholder\n",
    "            full_text = ' '.join(texts)\n",
    "            \n",
    "            return {\n",
    "                'text': full_text,\n",
    "                'confidence': 0.5,  # Placeholder as Keras-OCR doesn't provide confidence\n",
    "                'predictions': predictions\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {'text': '', 'confidence': 0, 'error': str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T09:17:00.419936Z",
     "start_time": "2025-10-16T09:17:00.404498Z"
    }
   },
   "outputs": [],
   "source": [
    "class TrOCRWrapper(OCRWrapper):\n",
    "    \"\"\"Wrapper for TrOCR (Transformer OCR)\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"TrOCR\")\n",
    "    \n",
    "    def initialize(self):\n",
    "        try:\n",
    "            from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
    "            \n",
    "            # Use small model for faster processing\n",
    "            self.processor = TrOCRProcessor.from_pretrained(\n",
    "                'microsoft/trocr-small-printed'\n",
    "            )\n",
    "            self.model = VisionEncoderDecoderModel.from_pretrained(\n",
    "                'microsoft/trocr-small-printed'\n",
    "            )\n",
    "            self.is_initialized = True\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to initialize TrOCR: {e}\")\n",
    "            self.is_initialized = False\n",
    "    \n",
    "    def process_image(self, image_path: str) -> Dict[str, Any]:\n",
    "        if not self.is_initialized:\n",
    "            return {'text': '', 'confidence': 0, 'error': 'Not initialized'}\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            \n",
    "            # Process image\n",
    "            pixel_values = self.processor(images=image, return_tensors=\"pt\").pixel_values\n",
    "            \n",
    "            # Generate text\n",
    "            generated_ids = self.model.generate(pixel_values)\n",
    "            generated_text = self.processor.batch_decode(\n",
    "                generated_ids, skip_special_tokens=True\n",
    "            )[0]\n",
    "            \n",
    "            return {\n",
    "                'text': generated_text,\n",
    "                'confidence': 0.7,  # Placeholder confidence\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {'text': '', 'confidence': 0, 'error': str(e)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T09:17:00.441856Z",
     "start_time": "2025-10-16T09:17:00.426301Z"
    }
   },
   "outputs": [],
   "source": [
    "class DocTRWrapper(OCRWrapper):\n",
    "    \"\"\"Wrapper for docTR\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\"docTR\")\n",
    "    \n",
    "    def initialize(self):\n",
    "        try:\n",
    "            from doctr.io import DocumentFile\n",
    "            from doctr.models import ocr_predictor\n",
    "            \n",
    "            self.model = ocr_predictor(pretrained=True)\n",
    "            self.DocumentFile = DocumentFile\n",
    "            self.is_initialized = True\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to initialize docTR: {e}\")\n",
    "            self.is_initialized = False\n",
    "    \n",
    "    def process_image(self, image_path: str) -> Dict[str, Any]:\n",
    "        if not self.is_initialized:\n",
    "            return {'text': '', 'confidence': 0, 'error': 'Not initialized'}\n",
    "        \n",
    "        try:\n",
    "            # Load document\n",
    "            doc = self.DocumentFile.from_images(image_path)\n",
    "            \n",
    "            # Perform OCR\n",
    "            result = self.model(doc)\n",
    "            \n",
    "            # Extract text\n",
    "            text_parts = []\n",
    "            confidences = []\n",
    "            \n",
    "            for page in result.pages:\n",
    "                for block in page.blocks:\n",
    "                    for line in block.lines:\n",
    "                        for word in line.words:\n",
    "                            text_parts.append(word.value)\n",
    "                            confidences.append(word.confidence)\n",
    "            \n",
    "            full_text = ' '.join(text_parts)\n",
    "            avg_confidence = np.mean(confidences) if confidences else 0\n",
    "            \n",
    "            return {\n",
    "                'text': full_text,\n",
    "                'confidence': avg_confidence,\n",
    "                'result': result\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {'text': '', 'confidence': 0, 'error': str(e)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Benchmark Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-16T09:17:00.474378Z",
     "start_time": "2025-10-16T09:17:00.452251Z"
    }
   },
   "outputs": [],
   "source": [
    "class BenchmarkRunner:\n",
    "    \"\"\"Run benchmarks for all OCR libraries\"\"\"\n",
    "    \n",
    "    def __init__(self, ocr_wrappers: List[OCRWrapper], test_data: pd.DataFrame):\n",
    "        self.ocr_wrappers = ocr_wrappers\n",
    "        self.test_data = test_data\n",
    "        self.results = []\n",
    "        self.metrics_calc = OCRMetrics()\n",
    "    \n",
    "    def run_single_test(self, ocr_wrapper: OCRWrapper, image_path: str, \n",
    "                       ground_truth: str) -> Dict[str, Any]:\n",
    "        \"\"\"Run a single OCR test\"\"\"\n",
    "        \n",
    "        # Measure processing time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Measure memory usage\n",
    "        def process_with_memory():\n",
    "            return ocr_wrapper.process_image(image_path)\n",
    "        \n",
    "        mem_usage = memory_usage(process_with_memory, interval=0.1, timeout=30)\n",
    "        result = ocr_wrapper.process_image(image_path)\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        # Extract text from result\n",
    "        predicted_text = result.get('text', '')\n",
    "        confidence = result.get('confidence', 0)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        cer = self.metrics_calc.character_error_rate(ground_truth, predicted_text)\n",
    "        wer = self.metrics_calc.word_error_rate(ground_truth, predicted_text)\n",
    "        accuracy = self.metrics_calc.accuracy(ground_truth, predicted_text)\n",
    "        pr_metrics = self.metrics_calc.precision_recall_f1(ground_truth, predicted_text)\n",
    "        \n",
    "        return {\n",
    "            'predicted_text': predicted_text,\n",
    "            'confidence': confidence,\n",
    "            'cer': cer,\n",
    "            'wer': wer,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': pr_metrics['precision'],\n",
    "            'recall': pr_metrics['recall'],\n",
    "            'f1': pr_metrics['f1'],\n",
    "            'processing_time': processing_time,\n",
    "            'memory_usage': max(mem_usage) if mem_usage else 0,\n",
    "            'error': result.get('error', None)\n",
    "        }\n",
    "    \n",
    "    def run_benchmarks(self):\n",
    "        \"\"\"Run complete benchmark suite\"\"\"\n",
    "        \n",
    "        print(\"Initializing OCR libraries...\")\n",
    "        for wrapper in self.ocr_wrappers:\n",
    "            print(f\"  Initializing {wrapper.name}...\")\n",
    "            wrapper.initialize()\n",
    "            if wrapper.is_initialized:\n",
    "                print(f\"    ✓ {wrapper.name} initialized successfully\")\n",
    "            else:\n",
    "                print(f\"    ✗ {wrapper.name} failed to initialize\")\n",
    "        \n",
    "        print(\"\\nRunning benchmarks...\")\n",
    "        \n",
    "        for idx, row in tqdm(self.test_data.iterrows(), total=len(self.test_data)):\n",
    "            image_path = row['path']\n",
    "            ground_truth = row['text']\n",
    "            category = row['category']\n",
    "            distortion = row['distortion']\n",
    "            \n",
    "            for wrapper in self.ocr_wrappers:\n",
    "                if not wrapper.is_initialized:\n",
    "                    continue\n",
    "                \n",
    "                try:\n",
    "                    test_result = self.run_single_test(wrapper, image_path, ground_truth)\n",
    "                    \n",
    "                    # Add metadata\n",
    "                    test_result.update({\n",
    "                        'library': wrapper.name,\n",
    "                        'image_path': image_path,\n",
    "                        'ground_truth': ground_truth,\n",
    "                        'category': category,\n",
    "                        'distortion': distortion\n",
    "                    })\n",
    "                    \n",
    "                    self.results.append(test_result)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"\\nError testing {wrapper.name} on {image_path}: {e}\")\n",
    "                    self.results.append({\n",
    "                        'library': wrapper.name,\n",
    "                        'image_path': image_path,\n",
    "                        'ground_truth': ground_truth,\n",
    "                        'category': category,\n",
    "                        'distortion': distortion,\n",
    "                        'error': str(e)\n",
    "                    })\n",
    "        \n",
    "        # Cleanup\n",
    "        for wrapper in self.ocr_wrappers:\n",
    "            wrapper.cleanup()\n",
    "        \n",
    "        return pd.DataFrame(self.results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-10-16T09:17:00.481294Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing OCR libraries...\n",
      "  Initializing Pytesseract...\n",
      "Failed to initialize Pytesseract: tesseract is not installed or it's not in your PATH. See README file for more information.\n",
      "    ✗ Pytesseract failed to initialize\n",
      "  Initializing EasyOCR...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CPU. Note: This module is much faster with a GPU.\n",
      "Downloading detection model, please wait. This may take several minutes depending upon your network connection.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading recognition model, please wait. This may take several minutes depending upon your network connection.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: |██████████████████████████████████████████████████| 100.0% Complete    ✓ EasyOCR initialized successfully\n",
      "  Initializing Keras-OCR...\n"
     ]
    }
   ],
   "source": [
    "# Initialize OCR wrappers\n",
    "ocr_wrappers = [\n",
    "    PytesseractWrapper(),\n",
    "    EasyOCRWrapper(),\n",
    "    KerasOCRWrapper(),\n",
    "    TrOCRWrapper(),\n",
    "    DocTRWrapper()\n",
    "]\n",
    "\n",
    "# Run benchmarks\n",
    "runner = BenchmarkRunner(ocr_wrappers, test_df)\n",
    "results_df = runner.run_benchmarks()\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv(config.output_dir / 'benchmark_results.csv', index=False)\n",
    "print(f\"\\nResults saved to {config.output_dir / 'benchmark_results.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results if needed\n",
    "# results_df = pd.read_csv(config.output_dir / 'benchmark_results.csv')\n",
    "\n",
    "# Clean data for analysis\n",
    "results_clean = results_df.dropna(subset=['cer', 'wer', 'processing_time'])\n",
    "\n",
    "# Summary statistics\n",
    "summary_stats = results_clean.groupby('library').agg({\n",
    "    'cer': ['mean', 'std', 'min', 'max'],\n",
    "    'wer': ['mean', 'std', 'min', 'max'],\n",
    "    'accuracy': 'mean',\n",
    "    'processing_time': ['mean', 'std'],\n",
    "    'memory_usage': 'mean',\n",
    "    'confidence': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BENCHMARK SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization functions\n",
    "def plot_performance_comparison():\n",
    "    \"\"\"Create comprehensive performance comparison plots\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    \n",
    "    # 1. CER Comparison\n",
    "    ax = axes[0, 0]\n",
    "    results_clean.boxplot(column='cer', by='library', ax=ax)\n",
    "    ax.set_title('Character Error Rate by Library')\n",
    "    ax.set_xlabel('Library')\n",
    "    ax.set_ylabel('CER (lower is better)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. WER Comparison\n",
    "    ax = axes[0, 1]\n",
    "    results_clean.boxplot(column='wer', by='library', ax=ax)\n",
    "    ax.set_title('Word Error Rate by Library')\n",
    "    ax.set_xlabel('Library')\n",
    "    ax.set_ylabel('WER (lower is better)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Processing Time\n",
    "    ax = axes[0, 2]\n",
    "    avg_time = results_clean.groupby('library')['processing_time'].mean().sort_values()\n",
    "    avg_time.plot(kind='barh', ax=ax, color='skyblue')\n",
    "    ax.set_title('Average Processing Time')\n",
    "    ax.set_xlabel('Time (seconds)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Accuracy by Category\n",
    "    ax = axes[1, 0]\n",
    "    pivot_accuracy = results_clean.pivot_table(\n",
    "        values='accuracy', \n",
    "        index='category', \n",
    "        columns='library', \n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    pivot_accuracy.plot(kind='bar', ax=ax)\n",
    "    ax.set_title('Accuracy by Text Category')\n",
    "    ax.set_xlabel('Category')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.legend(title='Library', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Performance vs Distortion\n",
    "    ax = axes[1, 1]\n",
    "    pivot_distortion = results_clean.pivot_table(\n",
    "        values='cer', \n",
    "        index='distortion', \n",
    "        columns='library', \n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    pivot_distortion.plot(kind='bar', ax=ax)\n",
    "    ax.set_title('CER by Distortion Type')\n",
    "    ax.set_xlabel('Distortion')\n",
    "    ax.set_ylabel('CER (lower is better)')\n",
    "    ax.legend(title='Library', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Memory Usage\n",
    "    ax = axes[1, 2]\n",
    "    avg_memory = results_clean.groupby('library')['memory_usage'].mean().sort_values()\n",
    "    avg_memory.plot(kind='barh', ax=ax, color='coral')\n",
    "    ax.set_title('Average Memory Usage')\n",
    "    ax.set_xlabel('Memory (MB)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('OCR Library Performance Comparison', fontsize=16, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(config.output_dir / 'performance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_performance_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a performance heatmap\n",
    "def plot_performance_heatmap():\n",
    "    \"\"\"Create a heatmap of normalized performance metrics\"\"\"\n",
    "    \n",
    "    # Prepare data for heatmap\n",
    "    metrics_for_heatmap = ['cer', 'wer', 'processing_time', 'memory_usage']\n",
    "    \n",
    "    # Calculate mean values for each metric\n",
    "    heatmap_data = results_clean.groupby('library')[metrics_for_heatmap].mean()\n",
    "    \n",
    "    # Normalize metrics (0-1 scale, where lower is better for all)\n",
    "    heatmap_norm = heatmap_data.copy()\n",
    "    for col in heatmap_norm.columns:\n",
    "        min_val = heatmap_norm[col].min()\n",
    "        max_val = heatmap_norm[col].max()\n",
    "        if max_val > min_val:\n",
    "            # Invert so that lower values are better (closer to 0)\n",
    "            heatmap_norm[col] = (heatmap_norm[col] - min_val) / (max_val - min_val)\n",
    "    \n",
    "    # Create heatmap\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(heatmap_norm.T, annot=True, fmt='.2f', cmap='RdYlGn_r', \n",
    "                cbar_kws={'label': 'Normalized Score (lower is better)'})\n",
    "    plt.title('OCR Library Performance Heatmap\\n(Normalized Metrics - Lower is Better)')\n",
    "    plt.xlabel('Library')\n",
    "    plt.ylabel('Metric')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(config.output_dir / 'performance_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_performance_heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a radar chart for overall comparison\n",
    "def plot_radar_chart():\n",
    "    \"\"\"Create radar chart for multi-dimensional comparison\"\"\"\n",
    "    \n",
    "    from math import pi\n",
    "    \n",
    "    # Select metrics for radar chart\n",
    "    metrics = ['Accuracy', 'Speed', 'Memory\\nEfficiency', 'Robustness', 'Confidence']\n",
    "    \n",
    "    # Calculate scores for each library\n",
    "    library_scores = {}\n",
    "    \n",
    "    for lib in results_clean['library'].unique():\n",
    "        lib_data = results_clean[results_clean['library'] == lib]\n",
    "        \n",
    "        # Calculate normalized scores (0-1, where 1 is best)\n",
    "        accuracy_score = lib_data['accuracy'].mean()\n",
    "        speed_score = 1 - (lib_data['processing_time'].mean() / \n",
    "                          results_clean['processing_time'].max())\n",
    "        memory_score = 1 - (lib_data['memory_usage'].mean() / \n",
    "                           results_clean['memory_usage'].max())\n",
    "        robustness_score = 1 - lib_data['cer'].mean()  # Inverse of error rate\n",
    "        confidence_score = lib_data['confidence'].mean()\n",
    "        \n",
    "        library_scores[lib] = [\n",
    "            accuracy_score,\n",
    "            speed_score,\n",
    "            memory_score,\n",
    "            robustness_score,\n",
    "            confidence_score\n",
    "        ]\n",
    "    \n",
    "    # Create radar chart\n",
    "    angles = [n / len(metrics) * 2 * pi for n in range(len(metrics))]\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "    \n",
    "    colors = plt.cm.Set2(range(len(library_scores)))\n",
    "    \n",
    "    for idx, (lib, scores) in enumerate(library_scores.items()):\n",
    "        scores += scores[:1]  # Complete the circle\n",
    "        ax.plot(angles, scores, 'o-', linewidth=2, label=lib, color=colors[idx])\n",
    "        ax.fill(angles, scores, alpha=0.25, color=colors[idx])\n",
    "    \n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(metrics)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "    ax.set_yticklabels(['0.2', '0.4', '0.6', '0.8', '1.0'])\n",
    "    ax.grid(True)\n",
    "    \n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "    plt.title('OCR Library Comparison - Radar Chart', size=16, y=1.08)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(config.output_dir / 'radar_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "plot_radar_chart()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Detailed Performance Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_performance_report():\n",
    "    \"\"\"Generate a detailed performance report\"\"\"\n",
    "    \n",
    "    report = []\n",
    "    report.append(\"=\"*80)\n",
    "    report.append(\"OCR LIBRARY BENCHMARK REPORT\")\n",
    "    report.append(\"=\"*80)\n",
    "    report.append(\"\")\n",
    "    \n",
    "    for lib in results_clean['library'].unique():\n",
    "        lib_data = results_clean[results_clean['library'] == lib]\n",
    "        \n",
    "        report.append(f\"\\n{lib}\")\n",
    "        report.append(\"-\" * len(lib))\n",
    "        \n",
    "        # Overall Performance\n",
    "        report.append(f\"Overall Accuracy: {lib_data['accuracy'].mean():.2%}\")\n",
    "        report.append(f\"Average CER: {lib_data['cer'].mean():.3f}\")\n",
    "        report.append(f\"Average WER: {lib_data['wer'].mean():.3f}\")\n",
    "        report.append(f\"Average Processing Time: {lib_data['processing_time'].mean():.3f}s\")\n",
    "        report.append(f\"Average Memory Usage: {lib_data['memory_usage'].mean():.1f} MB\")\n",
    "        report.append(f\"Average Confidence: {lib_data['confidence'].mean():.2%}\")\n",
    "        \n",
    "        # Performance by Category\n",
    "        report.append(\"\\nPerformance by Category:\")\n",
    "        for cat in lib_data['category'].unique():\n",
    "            cat_data = lib_data[lib_data['category'] == cat]\n",
    "            report.append(f\"  {cat}: Accuracy={cat_data['accuracy'].mean():.2%}, \"\n",
    "                         f\"CER={cat_data['cer'].mean():.3f}\")\n",
    "        \n",
    "        # Performance by Distortion\n",
    "        report.append(\"\\nPerformance by Distortion:\")\n",
    "        for dist in lib_data['distortion'].unique():\n",
    "            dist_data = lib_data[lib_data['distortion'] == dist]\n",
    "            report.append(f\"  {dist}: Accuracy={dist_data['accuracy'].mean():.2%}, \"\n",
    "                         f\"CER={dist_data['cer'].mean():.3f}\")\n",
    "        \n",
    "        # Strengths and Weaknesses\n",
    "        report.append(\"\\nStrengths:\")\n",
    "        if lib_data['processing_time'].mean() == results_clean.groupby('library')['processing_time'].mean().min():\n",
    "            report.append(\"  ✓ Fastest processing speed\")\n",
    "        if lib_data['memory_usage'].mean() == results_clean.groupby('library')['memory_usage'].mean().min():\n",
    "            report.append(\"  ✓ Most memory efficient\")\n",
    "        if lib_data['accuracy'].mean() == results_clean.groupby('library')['accuracy'].mean().max():\n",
    "            report.append(\"  ✓ Highest accuracy\")\n",
    "        if lib_data['cer'].mean() == results_clean.groupby('library')['cer'].mean().min():\n",
    "            report.append(\"  ✓ Lowest character error rate\")\n",
    "        \n",
    "        report.append(\"\")\n",
    "    \n",
    "    # Summary Recommendations\n",
    "    report.append(\"\\n\" + \"=\"*80)\n",
    "    report.append(\"RECOMMENDATIONS\")\n",
    "    report.append(\"=\"*80)\n",
    "    \n",
    "    # Find best library for each use case\n",
    "    best_accuracy = results_clean.groupby('library')['accuracy'].mean().idxmax()\n",
    "    best_speed = results_clean.groupby('library')['processing_time'].mean().idxmin()\n",
    "    best_memory = results_clean.groupby('library')['memory_usage'].mean().idxmin()\n",
    "    \n",
    "    report.append(f\"\\nBest for Accuracy: {best_accuracy}\")\n",
    "    report.append(f\"Best for Speed: {best_speed}\")\n",
    "    report.append(f\"Best for Memory Efficiency: {best_memory}\")\n",
    "    \n",
    "    # Handle distorted text\n",
    "    distorted_data = results_clean[results_clean['distortion'] != 'none']\n",
    "    best_robust = distorted_data.groupby('library')['cer'].mean().idxmin()\n",
    "    report.append(f\"Best for Distorted Text: {best_robust}\")\n",
    "    \n",
    "    report_text = \"\\n\".join(report)\n",
    "    \n",
    "    # Save report\n",
    "    with open(config.output_dir / 'benchmark_report.txt', 'w') as f:\n",
    "        f.write(report_text)\n",
    "    \n",
    "    print(report_text)\n",
    "    return report_text\n",
    "\n",
    "report = generate_performance_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results package\n",
    "def export_benchmark_results():\n",
    "    \"\"\"Export all benchmark results in various formats\"\"\"\n",
    "    \n",
    "    # 1. Detailed CSV\n",
    "    results_df.to_csv(config.output_dir / 'detailed_results.csv', index=False)\n",
    "    \n",
    "    # 2. Summary statistics\n",
    "    summary_df = results_clean.groupby('library').agg({\n",
    "        'accuracy': ['mean', 'std'],\n",
    "        'cer': ['mean', 'std'],\n",
    "        'wer': ['mean', 'std'],\n",
    "        'processing_time': ['mean', 'std'],\n",
    "        'memory_usage': ['mean', 'std'],\n",
    "        'confidence': ['mean', 'std']\n",
    "    })\n",
    "    summary_df.to_csv(config.output_dir / 'summary_statistics.csv')\n",
    "    \n",
    "    # 3. JSON format for programmatic access\n",
    "    results_json = {\n",
    "        'metadata': {\n",
    "            'total_tests': len(results_df),\n",
    "            'libraries_tested': list(results_df['library'].unique()),\n",
    "            'categories': list(results_df['category'].unique()),\n",
    "            'distortions': list(results_df['distortion'].unique())\n",
    "        },\n",
    "        'results': results_df.to_dict(orient='records'),\n",
    "        'summary': summary_df.to_dict()\n",
    "    }\n",
    "    \n",
    "    with open(config.output_dir / 'benchmark_results.json', 'w') as f:\n",
    "        json.dump(results_json, f, indent=2, default=str)\n",
    "    \n",
    "    # 4. Create a ranking table\n",
    "    ranking_data = []\n",
    "    for lib in results_clean['library'].unique():\n",
    "        lib_data = results_clean[results_clean['library'] == lib]\n",
    "        ranking_data.append({\n",
    "            'Library': lib,\n",
    "            'Accuracy Rank': 0,  # Will be filled\n",
    "            'Speed Rank': 0,\n",
    "            'Memory Rank': 0,\n",
    "            'CER Rank': 0,\n",
    "            'Overall Score': 0\n",
    "        })\n",
    "    \n",
    "    ranking_df = pd.DataFrame(ranking_data)\n",
    "    \n",
    "    # Calculate ranks\n",
    "    metrics_to_rank = {\n",
    "        'accuracy': 'Accuracy Rank',\n",
    "        'processing_time': 'Speed Rank',\n",
    "        'memory_usage': 'Memory Rank',\n",
    "        'cer': 'CER Rank'\n",
    "    }\n",
    "    \n",
    "    for metric, rank_col in metrics_to_rank.items():\n",
    "        lib_scores = results_clean.groupby('library')[metric].mean().sort_values()\n",
    "        if metric in ['processing_time', 'memory_usage', 'cer']:\n",
    "            lib_scores = lib_scores  # Lower is better\n",
    "        else:\n",
    "            lib_scores = lib_scores[::-1]  # Higher is better\n",
    "        \n",
    "        for rank, lib in enumerate(lib_scores.index, 1):\n",
    "            ranking_df.loc[ranking_df['Library'] == lib, rank_col] = rank\n",
    "    \n",
    "    # Calculate overall score (lower is better)\n",
    "    rank_cols = ['Accuracy Rank', 'Speed Rank', 'Memory Rank', 'CER Rank']\n",
    "    ranking_df['Overall Score'] = ranking_df[rank_cols].mean(axis=1)\n",
    "    ranking_df = ranking_df.sort_values('Overall Score')\n",
    "    ranking_df['Final Rank'] = range(1, len(ranking_df) + 1)\n",
    "    \n",
    "    ranking_df.to_csv(config.output_dir / 'library_rankings.csv', index=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL RANKINGS\")\n",
    "    print(\"=\"*80)\n",
    "    print(ranking_df.to_string(index=False))\n",
    "    \n",
    "    print(f\"\\nAll results exported to: {config.output_dir}\")\n",
    "    print(f\"  - detailed_results.csv\")\n",
    "    print(f\"  - summary_statistics.csv\")\n",
    "    print(f\"  - benchmark_results.json\")\n",
    "    print(f\"  - library_rankings.csv\")\n",
    "    print(f\"  - benchmark_report.txt\")\n",
    "    print(f\"  - performance_comparison.png\")\n",
    "    print(f\"  - performance_heatmap.png\")\n",
    "    print(f\"  - radar_comparison.png\")\n",
    "\n",
    "export_benchmark_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
