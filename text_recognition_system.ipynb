{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Recognition System for Distorted Images\n",
    "\n",
    "**Purpose:** Building a robust text recognition system using deep learning  \n",
    "**Framework:** TensorFlow/Keras  \n",
    "**Approach:** CNN-RNN hybrid architecture with CTC loss  \n",
    "\n",
    "This notebook demonstrates how to build a text recognition system that can handle distorted or challenging text images using a combination of convolutional and recurrent neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import ops\n",
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Keras version: {keras.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition and Preparation\n",
    "\n",
    "We'll work with a dataset of text images. Each image contains a sequence of characters that we need to recognize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download sample dataset\n",
    "!curl -LO https://github.com/AakashKumarNain/CaptchaCracker/raw/master/captcha_images_v2.zip\n",
    "!unzip -qq captcha_images_v2.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetConfig:\n",
    "    \"\"\"Configuration for dataset parameters\"\"\"\n",
    "    def __init__(self):\n",
    "        self.data_path = Path(\"./captcha_images_v2/\")\n",
    "        self.batch_size = 16\n",
    "        self.image_width = 200\n",
    "        self.image_height = 50\n",
    "        self.pool_factor = 4  # Downsampling through pooling layers\n",
    "        self.train_split = 0.9\n",
    "        self.validation_split = 0.1\n",
    "\n",
    "config = DatasetConfig()\n",
    "\n",
    "# Load and analyze dataset\n",
    "image_files = sorted(list(map(str, list(config.data_path.glob(\"*.png\")))))\n",
    "text_labels = [img.split(os.path.sep)[-1].split(\".png\")[0] for img in image_files]\n",
    "\n",
    "# Extract unique character set\n",
    "unique_chars = set(char for text in text_labels for char in text)\n",
    "unique_chars = sorted(list(unique_chars))\n",
    "\n",
    "# Dataset statistics\n",
    "print(f\"Dataset Statistics:\")\n",
    "print(f\"==================\")\n",
    "print(f\"Total samples: {len(image_files)}\")\n",
    "print(f\"Unique characters: {len(unique_chars)}\")\n",
    "print(f\"Character set: {unique_chars}\")\n",
    "print(f\"Max sequence length: {max([len(text) for text in text_labels])}\")\n",
    "print(f\"Min sequence length: {min([len(text) for text in text_labels])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Encoding System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterEncoder:\n",
    "    \"\"\"Handles character to integer mapping and vice versa\"\"\"\n",
    "    def __init__(self, characters):\n",
    "        self.char_to_int = layers.StringLookup(\n",
    "            vocabulary=list(characters), \n",
    "            mask_token=None\n",
    "        )\n",
    "        self.int_to_char = layers.StringLookup(\n",
    "            vocabulary=self.char_to_int.get_vocabulary(), \n",
    "            mask_token=None, \n",
    "            invert=True\n",
    "        )\n",
    "        self.vocab_size = len(self.char_to_int.get_vocabulary()) + 1\n",
    "    \n",
    "    def encode(self, text):\n",
    "        return self.char_to_int(tf.strings.unicode_split(text, input_encoding=\"UTF-8\"))\n",
    "    \n",
    "    def decode(self, integers):\n",
    "        return tf.strings.reduce_join(self.int_to_char(integers))\n",
    "\n",
    "encoder = CharacterEncoder(unique_chars)\n",
    "max_text_len = max([len(text) for text in text_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pipeline Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_val_split(images, labels, split_ratio=0.9, shuffle=True):\n",
    "    \"\"\"Split dataset into training and validation sets\"\"\"\n",
    "    dataset_size = len(images)\n",
    "    indices = ops.arange(dataset_size)\n",
    "    \n",
    "    if shuffle:\n",
    "        indices = keras.random.shuffle(indices)\n",
    "    \n",
    "    split_point = int(dataset_size * split_ratio)\n",
    "    \n",
    "    train_images = images[indices[:split_point]]\n",
    "    train_labels = labels[indices[:split_point]]\n",
    "    val_images = images[indices[split_point:]]\n",
    "    val_labels = labels[indices[split_point:]]\n",
    "    \n",
    "    return train_images, val_images, train_labels, val_labels\n",
    "\n",
    "# Create splits\n",
    "train_imgs, val_imgs, train_lbls, val_lbls = create_train_val_split(\n",
    "    np.array(image_files), \n",
    "    np.array(text_labels),\n",
    "    split_ratio=config.train_split\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_imgs)}\")\n",
    "print(f\"Validation samples: {len(val_imgs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path, label):\n",
    "    \"\"\"Preprocess individual image and label pair\"\"\"\n",
    "    # Load and process image\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.io.decode_png(image, channels=1)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    image = ops.image.resize(image, [config.image_height, config.image_width])\n",
    "    \n",
    "    # Transpose for sequence processing (width becomes time dimension)\n",
    "    image = ops.transpose(image, axes=[1, 0, 2])\n",
    "    \n",
    "    # Encode text label\n",
    "    label = encoder.encode(label)\n",
    "    \n",
    "    return {\"image\": image, \"label\": label}\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_data = tf.data.Dataset.from_tensor_slices((train_imgs, train_lbls))\n",
    "train_data = (\n",
    "    train_data\n",
    "    .map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(config.batch_size)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "val_data = tf.data.Dataset.from_tensor_slices((val_imgs, val_lbls))\n",
    "val_data = (\n",
    "    val_data\n",
    "    .map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(config.batch_size)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_samples(dataset, num_samples=16):\n",
    "    \"\"\"Visualize sample images with their labels\"\"\"\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(12, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for batch in dataset.take(1):\n",
    "        images = batch[\"image\"]\n",
    "        labels = batch[\"label\"]\n",
    "        \n",
    "        for idx in range(min(num_samples, len(images))):\n",
    "            img_array = (images[idx] * 255).numpy().astype(\"uint8\")\n",
    "            label_text = encoder.decode(labels[idx]).numpy().decode(\"utf-8\")\n",
    "            \n",
    "            axes[idx].imshow(img_array[:, :, 0].T, cmap='gray')\n",
    "            axes[idx].set_title(f\"Text: {label_text}\", fontsize=10)\n",
    "            axes[idx].axis('off')\n",
    "    \n",
    "    plt.suptitle(\"Sample Images from Dataset\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_samples(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Loss Implementation\n",
    "\n",
    "We implement Connectionist Temporal Classification (CTC) loss for sequence recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ctc_loss(y_true, y_pred, input_length, label_length):\n",
    "    \"\"\"Compute CTC loss for batch\"\"\"\n",
    "    label_length = ops.cast(ops.squeeze(label_length, axis=-1), dtype=\"int32\")\n",
    "    input_length = ops.cast(ops.squeeze(input_length, axis=-1), dtype=\"int32\")\n",
    "    \n",
    "    # Convert dense labels to sparse format\n",
    "    sparse_labels = dense_to_sparse(y_true, label_length)\n",
    "    \n",
    "    # Transpose predictions for CTC loss computation\n",
    "    y_pred = ops.log(ops.transpose(y_pred, axes=[1, 0, 2]) + keras.backend.epsilon())\n",
    "    \n",
    "    # Compute CTC loss\n",
    "    loss = tf.compat.v1.nn.ctc_loss(\n",
    "        inputs=y_pred, \n",
    "        labels=sparse_labels, \n",
    "        sequence_length=input_length\n",
    "    )\n",
    "    \n",
    "    return ops.expand_dims(loss, 1)\n",
    "\n",
    "def dense_to_sparse(labels, label_lengths):\n",
    "    \"\"\"Convert dense label tensor to sparse format for CTC\"\"\"\n",
    "    label_shape = ops.shape(labels)\n",
    "    batch_size = label_shape[0]\n",
    "    max_length = label_shape[1]\n",
    "    \n",
    "    # Create mask for valid label positions\n",
    "    indices = []\n",
    "    values = []\n",
    "    \n",
    "    for batch_idx in range(batch_size):\n",
    "        for time_idx in range(label_lengths[batch_idx]):\n",
    "            indices.append([batch_idx, time_idx])\n",
    "            values.append(labels[batch_idx, time_idx])\n",
    "    \n",
    "    # Create sparse tensor\n",
    "    indices = ops.cast(indices, dtype=\"int64\")\n",
    "    values = ops.cast(values, dtype=\"int32\")\n",
    "    shape = ops.cast([batch_size, max_length], dtype=\"int64\")\n",
    "    \n",
    "    return tf.SparseTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTCLossLayer(layers.Layer):\n",
    "    \"\"\"Custom layer for CTC loss computation during training\"\"\"\n",
    "    \n",
    "    def __init__(self, name=\"ctc_loss_layer\"):\n",
    "        super().__init__(name=name)\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        batch_size = ops.shape(y_true)[0]\n",
    "        max_time = ops.shape(y_pred)[1]\n",
    "        label_length = ops.shape(y_true)[1]\n",
    "        \n",
    "        # Create input length tensor (all sequences use full length)\n",
    "        input_length = max_time * ops.ones(shape=(batch_size, 1), dtype=\"int32\")\n",
    "        label_length = label_length * ops.ones(shape=(batch_size, 1), dtype=\"int32\")\n",
    "        \n",
    "        # Compute and add loss\n",
    "        loss = compute_ctc_loss(y_true, y_pred, input_length, label_length)\n",
    "        self.add_loss(loss)\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "We design a hybrid CNN-RNN architecture:\n",
    "- CNN layers extract visual features\n",
    "- RNN layers process the sequence\n",
    "- CTC loss handles alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_recognition_model():\n",
    "    \"\"\"Build the text recognition model\"\"\"\n",
    "    \n",
    "    # Input layers\n",
    "    image_input = layers.Input(\n",
    "        shape=(config.image_width, config.image_height, 1), \n",
    "        name=\"image\", \n",
    "        dtype=\"float32\"\n",
    "    )\n",
    "    label_input = layers.Input(\n",
    "        name=\"label\", \n",
    "        shape=(None,), \n",
    "        dtype=\"float32\"\n",
    "    )\n",
    "    \n",
    "    # Feature extraction with CNN\n",
    "    x = image_input\n",
    "    \n",
    "    # First convolutional block\n",
    "    x = layers.Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=(3, 3),\n",
    "        activation=\"relu\",\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        padding=\"same\",\n",
    "        name=\"conv_block1\"\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization(name=\"bn1\")(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2), name=\"pool1\")(x)\n",
    "    \n",
    "    # Second convolutional block\n",
    "    x = layers.Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=(3, 3),\n",
    "        activation=\"relu\",\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        padding=\"same\",\n",
    "        name=\"conv_block2\"\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization(name=\"bn2\")(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2), name=\"pool2\")(x)\n",
    "    \n",
    "    # Third convolutional block (additional depth)\n",
    "    x = layers.Conv2D(\n",
    "        filters=128,\n",
    "        kernel_size=(3, 3),\n",
    "        activation=\"relu\",\n",
    "        kernel_initializer=\"he_normal\",\n",
    "        padding=\"same\",\n",
    "        name=\"conv_block3\"\n",
    "    )(x)\n",
    "    x = layers.BatchNormalization(name=\"bn3\")(x)\n",
    "    \n",
    "    # Reshape for RNN processing\n",
    "    feature_dims = (config.image_width // config.pool_factor, \n",
    "                   (config.image_height // config.pool_factor) * 128)\n",
    "    x = layers.Reshape(target_shape=feature_dims, name=\"reshape_features\")(x)\n",
    "    \n",
    "    # Dense layer for feature transformation\n",
    "    x = layers.Dense(128, activation=\"relu\", name=\"feature_dense\")(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    # Bidirectional LSTM layers for sequence processing\n",
    "    x = layers.Bidirectional(\n",
    "        layers.LSTM(256, return_sequences=True, dropout=0.2),\n",
    "        name=\"bi_lstm1\"\n",
    "    )(x)\n",
    "    x = layers.Bidirectional(\n",
    "        layers.LSTM(128, return_sequences=True, dropout=0.2),\n",
    "        name=\"bi_lstm2\"\n",
    "    )(x)\n",
    "    \n",
    "    # Output layer with character predictions\n",
    "    x = layers.Dense(\n",
    "        encoder.vocab_size, \n",
    "        activation=\"softmax\", \n",
    "        name=\"character_output\"\n",
    "    )(x)\n",
    "    \n",
    "    # Add CTC loss layer\n",
    "    output = CTCLossLayer()(label_input, x)\n",
    "    \n",
    "    # Create and compile model\n",
    "    model = keras.models.Model(\n",
    "        inputs=[image_input, label_input], \n",
    "        outputs=output, \n",
    "        name=\"text_recognition_model\"\n",
    "    )\n",
    "    \n",
    "    # Use Adam optimizer with learning rate scheduling\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "recognition_model = create_recognition_model()\n",
    "recognition_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingConfig:\n",
    "    \"\"\"Training hyperparameters and callbacks\"\"\"\n",
    "    def __init__(self):\n",
    "        self.epochs = 50\n",
    "        self.patience = 10\n",
    "        self.reduce_lr_patience = 5\n",
    "        self.checkpoint_path = \"best_model.keras\"\n",
    "\n",
    "train_config = TrainingConfig()\n",
    "\n",
    "# Define callbacks\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=train_config.patience,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\",\n",
    "        factor=0.5,\n",
    "        patience=train_config.reduce_lr_patience,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    ),\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=train_config.checkpoint_path,\n",
    "        monitor=\"val_loss\",\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "training_history = recognition_model.fit(\n",
    "    train_data,\n",
    "    validation_data=val_data,\n",
    "    epochs=train_config.epochs,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"Visualize training metrics\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Plot loss\n",
    "    axes[0].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "    axes[0].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].set_title('Model Loss Over Time')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot learning rate if available\n",
    "    if 'lr' in history.history:\n",
    "        axes[1].plot(history.history['lr'], label='Learning Rate', linewidth=2, color='green')\n",
    "        axes[1].set_xlabel('Epoch')\n",
    "        axes[1].set_ylabel('Learning Rate')\n",
    "        axes[1].set_title('Learning Rate Schedule')\n",
    "        axes[1].legend()\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(training_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_ctc_output(y_pred, input_length, use_beam_search=False, beam_width=100):\n",
    "    \"\"\"Decode CTC output to text\"\"\"\n",
    "    batch_size = ops.shape(y_pred)[0]\n",
    "    max_length = ops.shape(y_pred)[1]\n",
    "    \n",
    "    # Transpose for CTC decoder\n",
    "    y_pred = ops.log(ops.transpose(y_pred, axes=[1, 0, 2]) + keras.backend.epsilon())\n",
    "    input_length = ops.cast(input_length, dtype=\"int32\")\n",
    "    \n",
    "    if use_beam_search:\n",
    "        decoded, log_prob = tf.compat.v1.nn.ctc_beam_search_decoder(\n",
    "            inputs=y_pred,\n",
    "            sequence_length=input_length,\n",
    "            beam_width=beam_width,\n",
    "            top_paths=1\n",
    "        )\n",
    "    else:\n",
    "        decoded, log_prob = tf.nn.ctc_greedy_decoder(\n",
    "            inputs=y_pred,\n",
    "            sequence_length=input_length\n",
    "        )\n",
    "    \n",
    "    # Convert sparse to dense\n",
    "    sparse_tensor = decoded[0]\n",
    "    dense_tensor = tf.SparseTensor(\n",
    "        sparse_tensor.indices, \n",
    "        sparse_tensor.values, \n",
    "        (batch_size, max_length)\n",
    "    )\n",
    "    decoded_dense = tf.sparse.to_dense(sp_input=dense_tensor, default_value=-1)\n",
    "    \n",
    "    return decoded_dense, log_prob\n",
    "\n",
    "# Create prediction model (without CTC loss layer)\n",
    "prediction_model = keras.models.Model(\n",
    "    recognition_model.input[0], \n",
    "    recognition_model.get_layer(name=\"character_output\").output\n",
    ")\n",
    "\n",
    "def predict_text(predictions):\n",
    "    \"\"\"Convert model predictions to text\"\"\"\n",
    "    input_lengths = np.ones(predictions.shape[0]) * predictions.shape[1]\n",
    "    \n",
    "    # Decode predictions\n",
    "    decoded_outputs, _ = decode_ctc_output(\n",
    "        predictions, \n",
    "        input_length=input_lengths, \n",
    "        use_beam_search=False\n",
    "    )\n",
    "    \n",
    "    # Convert to text\n",
    "    predicted_texts = []\n",
    "    for sequence in decoded_outputs[:, :max_text_len]:\n",
    "        text = encoder.decode(sequence).numpy().decode(\"utf-8\")\n",
    "        predicted_texts.append(text)\n",
    "    \n",
    "    return predicted_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(dataset, num_batches=1):\n",
    "    \"\"\"Evaluate model performance on dataset\"\"\"\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    sample_idx = 0\n",
    "    \n",
    "    for batch in dataset.take(num_batches):\n",
    "        batch_images = batch[\"image\"]\n",
    "        batch_labels = batch[\"label\"]\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = prediction_model.predict(batch_images, verbose=0)\n",
    "        predicted_texts = predict_text(predictions)\n",
    "        \n",
    "        # Get ground truth\n",
    "        true_texts = []\n",
    "        for label in batch_labels:\n",
    "            text = encoder.decode(label).numpy().decode(\"utf-8\")\n",
    "            true_texts.append(text)\n",
    "        \n",
    "        # Visualize results\n",
    "        for i in range(min(len(predicted_texts), 16 - sample_idx)):\n",
    "            if sample_idx >= 16:\n",
    "                break\n",
    "                \n",
    "            img = (batch_images[i, :, :, 0] * 255).numpy().astype(np.uint8)\n",
    "            img = img.T\n",
    "            \n",
    "            # Color code based on accuracy\n",
    "            color = 'green' if predicted_texts[i] == true_texts[i] else 'red'\n",
    "            \n",
    "            axes[sample_idx].imshow(img, cmap='gray')\n",
    "            axes[sample_idx].set_title(\n",
    "                f\"True: {true_texts[i]}\\nPred: {predicted_texts[i]}\", \n",
    "                fontsize=9, \n",
    "                color=color\n",
    "            )\n",
    "            axes[sample_idx].axis('off')\n",
    "            sample_idx += 1\n",
    "    \n",
    "    plt.suptitle(\"Model Predictions (Green=Correct, Red=Incorrect)\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Evaluate on validation set\n",
    "print(\"Evaluating on validation set...\")\n",
    "evaluate_model(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(dataset, max_batches=None):\n",
    "    \"\"\"Calculate character and sequence accuracy\"\"\"\n",
    "    total_sequences = 0\n",
    "    correct_sequences = 0\n",
    "    total_chars = 0\n",
    "    correct_chars = 0\n",
    "    \n",
    "    batch_count = 0\n",
    "    for batch in dataset:\n",
    "        if max_batches and batch_count >= max_batches:\n",
    "            break\n",
    "            \n",
    "        batch_images = batch[\"image\"]\n",
    "        batch_labels = batch[\"label\"]\n",
    "        \n",
    "        # Get predictions\n",
    "        predictions = prediction_model.predict(batch_images, verbose=0)\n",
    "        predicted_texts = predict_text(predictions)\n",
    "        \n",
    "        # Get ground truth\n",
    "        for i, label in enumerate(batch_labels):\n",
    "            true_text = encoder.decode(label).numpy().decode(\"utf-8\")\n",
    "            pred_text = predicted_texts[i]\n",
    "            \n",
    "            # Sequence accuracy\n",
    "            total_sequences += 1\n",
    "            if pred_text == true_text:\n",
    "                correct_sequences += 1\n",
    "            \n",
    "            # Character accuracy\n",
    "            for j in range(min(len(true_text), len(pred_text))):\n",
    "                total_chars += 1\n",
    "                if j < len(pred_text) and true_text[j] == pred_text[j]:\n",
    "                    correct_chars += 1\n",
    "            total_chars += abs(len(true_text) - len(pred_text))\n",
    "        \n",
    "        batch_count += 1\n",
    "    \n",
    "    seq_accuracy = correct_sequences / total_sequences if total_sequences > 0 else 0\n",
    "    char_accuracy = correct_chars / total_chars if total_chars > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'sequence_accuracy': seq_accuracy,\n",
    "        'character_accuracy': char_accuracy,\n",
    "        'total_sequences': total_sequences,\n",
    "        'correct_sequences': correct_sequences\n",
    "    }\n",
    "\n",
    "# Calculate metrics\n",
    "print(\"Calculating performance metrics...\")\n",
    "train_metrics = calculate_accuracy(train_data, max_batches=10)\n",
    "val_metrics = calculate_accuracy(val_data)\n",
    "\n",
    "print(\"\\nTraining Set Performance:\")\n",
    "print(f\"  Sequence Accuracy: {train_metrics['sequence_accuracy']:.2%}\")\n",
    "print(f\"  Character Accuracy: {train_metrics['character_accuracy']:.2%}\")\n",
    "\n",
    "print(\"\\nValidation Set Performance:\")\n",
    "print(f\"  Sequence Accuracy: {val_metrics['sequence_accuracy']:.2%}\")\n",
    "print(f\"  Character Accuracy: {val_metrics['character_accuracy']:.2%}\")\n",
    "print(f\"  Correct Sequences: {val_metrics['correct_sequences']}/{val_metrics['total_sequences']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Export and Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_save_path = \"text_recognition_model.keras\"\n",
    "prediction_model.save(model_save_path)\n",
    "print(f\"Model saved to: {model_save_path}\")\n",
    "\n",
    "# Save configuration\n",
    "import json\n",
    "\n",
    "config_dict = {\n",
    "    'image_width': config.image_width,\n",
    "    'image_height': config.image_height,\n",
    "    'vocab_size': encoder.vocab_size,\n",
    "    'characters': unique_chars,\n",
    "    'max_text_length': max_text_len\n",
    "}\n",
    "\n",
    "with open('model_config.json', 'w') as f:\n",
    "    json.dump(config_dict, f, indent=2)\n",
    "print(\"Configuration saved to: model_config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_image(image_path):\n",
    "    \"\"\"Predict text from a single image\"\"\"\n",
    "    # Load and preprocess image\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.io.decode_png(image, channels=1)\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    image = ops.image.resize(image, [config.image_height, config.image_width])\n",
    "    image = ops.transpose(image, axes=[1, 0, 2])\n",
    "    image = ops.expand_dims(image, axis=0)  # Add batch dimension\n",
    "    \n",
    "    # Get prediction\n",
    "    prediction = prediction_model.predict(image, verbose=0)\n",
    "    predicted_text = predict_text(prediction)[0]\n",
    "    \n",
    "    return predicted_text\n",
    "\n",
    "# Test on a sample image\n",
    "sample_image_path = image_files[0]\n",
    "predicted = predict_single_image(sample_image_path)\n",
    "actual = text_labels[0]\n",
    "\n",
    "print(f\"Sample Image: {sample_image_path}\")\n",
    "print(f\"Actual Text: {actual}\")\n",
    "print(f\"Predicted Text: {predicted}\")\n",
    "print(f\"Match: {'✓' if predicted == actual else '✗'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
